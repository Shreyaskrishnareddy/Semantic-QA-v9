{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u26a0\ufe0f Security Notice\n",
        "\n",
        "API keys and secrets have been sanitized from this notebook for security.\n",
        "Please replace placeholder values with your actual API keys before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5SxAnD-J5GP"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu sentence-transformers beautifulsoup4 lxml PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install xbrl\n",
        "!pip install beautifulsoup4 lxml\n",
        "!pip install pymupdf\n",
        "!pip install langchain huggingface_hub\n",
        "!pip install -U langchain-huggingface"
      ],
      "metadata": {
        "id": "5mGIvEFaK4WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install -U openai langchain"
      ],
      "metadata": {
        "id": "D2gByGe8K4Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-YOUR_OPENAI_PROJECT_KEY_HERE\"\n"
      ],
      "metadata": {
        "id": "hK08A3xZK4Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.2\n",
        ")\n"
      ],
      "metadata": {
        "id": "zsasf5VwK4Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okeK9IjPK4L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core imports\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from bs4 import BeautifulSoup\n",
        "import xml.etree.ElementTree as ET\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "K6OEeFPpJ7YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_pdf(file_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def parse_html(file_path):\n",
        "    \"\"\"Extracts text from an HTML/HTM file.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f, \"lxml\")\n",
        "        return soup.get_text(separator=\"\\n\")\n",
        "\n",
        "def parse_xml(file_path):\n",
        "    \"\"\"Extracts text from an XML file.\"\"\"\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    text = ET.tostring(root, encoding=\"unicode\", method=\"text\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "z7HTas5qJ7Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    \"\"\"\n",
        "    Splits text into overlapping chunks.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "Z9c66JheJ7Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks):\n",
        "    \"\"\"\n",
        "    Converts text chunks to embeddings using sentence-transformers.\n",
        "    \"\"\"\n",
        "    embeddings = embedder.encode(\n",
        "        chunks,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "47g5J3YkJ7P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload multiple files\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Map file extensions to parser functions\n",
        "parser_map = {\n",
        "    \".pdf\": parse_pdf,\n",
        "    \".htm\": parse_html,\n",
        "    \".html\": parse_html,\n",
        "    \".xml\": parse_xml\n",
        "}\n",
        "\n",
        "# Helper to choose parser based on extension\n",
        "def get_parser(file_name):\n",
        "    ext = os.path.splitext(file_name)[1].lower()\n",
        "    return parser_map.get(ext, None)"
      ],
      "metadata": {
        "id": "aDAPslgOJ7Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunks = []\n",
        "all_embeddings = []\n",
        "all_metadata = []\n",
        "\n",
        "for file_name in uploaded_files.keys():\n",
        "    parser = get_parser(file_name)\n",
        "    if parser is None:\n",
        "        print(f\"\u26a0\ufe0f Skipping unsupported file: {file_name}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\u2705 Processing: {file_name}\")\n",
        "\n",
        "    # Parse text\n",
        "    text = parser(file_name)\n",
        "\n",
        "    # Chunk text\n",
        "    chunks = chunk_text(text, chunk_size=500, overlap=50)\n",
        "\n",
        "    # Embed chunks\n",
        "    embeddings = embed_chunks(chunks)\n",
        "\n",
        "    # Collect\n",
        "    all_chunks.extend(chunks)\n",
        "    all_embeddings.append(embeddings)\n",
        "\n",
        "    # Store metadata for each chunk\n",
        "    all_metadata.extend([\n",
        "        {\"file_name\": file_name, \"chunk_id\": idx}\n",
        "        for idx in range(len(chunks))\n",
        "    ])\n",
        "\n",
        "print(\"\u2705 All files processed.\")\n"
      ],
      "metadata": {
        "id": "AjU7i88NJ7Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack all embeddings into one big matrix\n",
        "embedding_matrix = np.vstack(all_embeddings)\n",
        "\n",
        "# Create FAISS index\n",
        "dimension = embedding_matrix.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embedding_matrix)\n",
        "\n",
        "print(f\"\u2705 FAISS index created with {index.ntotal} vectors.\")\n"
      ],
      "metadata": {
        "id": "xn00da4FJ7JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, k=5):\n",
        "    \"\"\"\n",
        "    Embeds the query and retrieves top-k most similar chunks with metadata.\n",
        "    \"\"\"\n",
        "    # Embed the query\n",
        "    q_embedding = embedder.encode([query])\n",
        "\n",
        "    # Search FAISS\n",
        "    distances, indices = index.search(np.array(q_embedding), k)\n",
        "\n",
        "    # Collect results\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        result = {\n",
        "            \"text\": all_chunks[idx],\n",
        "            \"metadata\": all_metadata[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "HmdvRLeQJ7G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(results):\n",
        "    \"\"\"\n",
        "    Pretty-print retrieved chunks with metadata.\n",
        "    \"\"\"\n",
        "    for i, r in enumerate(results, start=1):\n",
        "        print(f\"\\n\ud83d\udd39 Result #{i}\")\n",
        "        print(f\"\ud83d\udcc4 File: {r['metadata']['file_name']} | Chunk ID: {r['metadata']['chunk_id']}\")\n",
        "        print(\"------\")\n",
        "        snippet = r[\"text\"][:500]  # Show only first 500 characters\n",
        "        print(snippet)\n"
      ],
      "metadata": {
        "id": "k0PRHnx-J7Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"What are Apple's supply chain risks?\"\n",
        "\n",
        "# Run search\n",
        "results = semantic_search(query, k=5)\n",
        "\n",
        "# Display results\n",
        "display_results(results)\n"
      ],
      "metadata": {
        "id": "aBhtGE-4J7CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize your LLM (change model if needed)\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "def summarize_with_llm(query, results):\n",
        "    \"\"\"\n",
        "    Combines retrieved chunks and sends them to the LLM to generate an answer.\n",
        "    \"\"\"\n",
        "    # Build context with citations\n",
        "    context = \"\"\n",
        "    for r in results:\n",
        "        snippet = r[\"text\"][:1000].strip().replace(\"\\n\", \" \")\n",
        "        file = r[\"metadata\"][\"file_name\"]\n",
        "        context += f\"\\n(Source: {file})\\n{snippet}\\n\"\n",
        "\n",
        "    # Prompt\n",
        "    prompt = (\n",
        "        f\"You are a financial analyst assistant.\\n\\n\"\n",
        "        f\"Context excerpts:\\n{context}\\n\\n\"\n",
        "        f\"Question: {query}\\n\\n\"\n",
        "        f\"Answer in bullet points with clear sentences.\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content.strip()\n"
      ],
      "metadata": {
        "id": "68zbP_rTJ6__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example question\n",
        "query = \"Summarize Apple's supply chain risks.\"\n",
        "\n",
        "# Retrieve top 5 chunks\n",
        "results = semantic_search(query, k=5)\n",
        "\n",
        "# Generate answer\n",
        "summary = summarize_with_llm(query, results)\n",
        "\n",
        "# Display answer\n",
        "print(\"\u2705 LLM Summary:\\n\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "72rFFf9aKJN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2\n",
        "query2 = \"What were the key financial highlights in 2023?\"\n",
        "results2 = semantic_search(query2, k=5)\n",
        "summary2 = summarize_with_llm(query2, results2)\n",
        "print(\"\u2705 Financial Highlights Summary:\\n\")\n",
        "print(summary2)\n"
      ],
      "metadata": {
        "id": "joEgwOoeKJKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "\n",
        "def extract_xbrl_facts(xml_file):\n",
        "    \"\"\"\n",
        "    Extracts all us-gaap facts and their values from an XBRL XML file.\n",
        "    Returns a list of dicts with label and value.\n",
        "    \"\"\"\n",
        "    tree = etree.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    ns = {\"us-gaap\": \"http://fasb.org/us-gaap/2023\"}\n",
        "\n",
        "    # Find all tags in the us-gaap namespace\n",
        "    facts = []\n",
        "    for element in root.iter():\n",
        "        if element.tag.startswith(\"{http://fasb.org/us-gaap/2023}\"):\n",
        "            tag = element.tag.split(\"}\")[1]\n",
        "            value = element.text\n",
        "            facts.append({\n",
        "                \"Label\": tag,\n",
        "                \"Value\": value\n",
        "            })\n",
        "    return facts\n"
      ],
      "metadata": {
        "id": "cRMG3G1jKJIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your uploaded XBRL file\n",
        "xbrl_file = \"aapl-20230930_htm.xml\"\n",
        "\n",
        "# Extract all facts\n",
        "facts = extract_xbrl_facts(xbrl_file)\n",
        "\n",
        "# Show a sample of what we got\n",
        "print(f\"\u2705 Extracted {len(facts)} facts. Showing first 10:\\n\")\n",
        "for f in facts[:10]:\n",
        "    print(f)\n"
      ],
      "metadata": {
        "id": "KiAXHiJwKVqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tags we care about\n",
        "important_tags = {\n",
        "    \"RevenueFromContractWithCustomerExcludingAssessedTax\": \"Revenue\",\n",
        "    \"Revenues\": \"Revenue\",\n",
        "    \"SalesRevenueNet\": \"Revenue\",\n",
        "    \"CostOfGoodsAndServicesSold\": \"COGS\",\n",
        "    \"OperatingIncomeLoss\": \"OperatingIncome\",\n",
        "    \"NetIncomeLoss\": \"NetIncome\",\n",
        "    \"Assets\": \"TotalAssets\",\n",
        "    \"Liabilities\": \"TotalLiabilities\",\n",
        "    \"StockholdersEquity\": \"Equity\",\n",
        "    \"LongTermDebt\": \"LongTermDebt\",\n",
        "}\n",
        "\n",
        "# Initialize empty dict\n",
        "data = {}\n",
        "\n",
        "# Loop through extracted facts\n",
        "for f in facts:\n",
        "    tag = f[\"Label\"]\n",
        "    value = f[\"Value\"]\n",
        "    if tag in important_tags:\n",
        "        try:\n",
        "            # Convert to numeric safely\n",
        "            numeric_value = float(value)\n",
        "            data[important_tags[tag]] = numeric_value\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# Display cleaned data\n",
        "print(\"\u2705 Extracted Financial Metrics:\\n\")\n",
        "for k, v in data.items():\n",
        "    print(f\"{k}: {v:,.0f}\")\n"
      ],
      "metadata": {
        "id": "h2XgPNAtKVnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert dict to DataFrame\n",
        "df = pd.DataFrame([data])\n",
        "\n",
        "# Compute Ratios\n",
        "df[\"GrossMargin\"] = (df[\"Revenue\"] - df[\"COGS\"]) / df[\"Revenue\"] * 100\n",
        "df[\"OperatingMargin\"] = df[\"OperatingIncome\"] / df[\"Revenue\"] * 100\n",
        "df[\"NetMargin\"] = df[\"NetIncome\"] / df[\"Revenue\"] * 100\n",
        "df[\"DebtEquity\"] = df[\"LongTermDebt\"] / df[\"Equity\"]\n",
        "df[\"LiabilitiesEquity\"] = df[\"TotalLiabilities\"] / df[\"Equity\"]\n",
        "\n",
        "# Display nicely\n",
        "print(\"\u2705 Financial Ratios:\\n\")\n",
        "print(df[[\n",
        "    \"GrossMargin\",\n",
        "    \"OperatingMargin\",\n",
        "    \"NetMargin\",\n",
        "    \"DebtEquity\",\n",
        "    \"LiabilitiesEquity\"\n",
        "]].round(2))\n"
      ],
      "metadata": {
        "id": "HPQ0GI7vKVk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create figure and axes\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Bar plot of the ratios\n",
        "bars = ax.bar(\n",
        "    [\"Gross Margin\", \"Operating Margin\", \"Net Margin\"],\n",
        "    [\n",
        "        df[\"GrossMargin\"].iloc[0],\n",
        "        df[\"OperatingMargin\"].iloc[0],\n",
        "        df[\"NetMargin\"].iloc[0]\n",
        "    ],\n",
        "    color=\"#5fba7d\"\n",
        ")\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f\"{yval:.2f}%\", ha='center', fontsize=12)\n",
        "\n",
        "# Customize plot\n",
        "ax.set_title(\"Apple Financial Margins (%)\", fontsize=16)\n",
        "ax.set_ylabel(\"Percentage\")\n",
        "ax.set_ylim(0, max(df[\"GrossMargin\"].iloc[0], df[\"OperatingMargin\"].iloc[0], df[\"NetMargin\"].iloc[0]) + 10)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UHs__IwMKVid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "file = \"/content/aapl-20210925_htm.xml\"\n",
        "\n",
        "with open(file, \"r\") as f:\n",
        "    soup = BeautifulSoup(f, \"lxml\")\n",
        "\n",
        "# List unique tag names\n",
        "all_tags = set([tag.name for tag in soup.find_all()])\n",
        "\n",
        "print(\"\u2705 Unique tag names in 2021 file:\")\n",
        "for t in sorted(all_tags):\n",
        "    print(t)\n"
      ],
      "metadata": {
        "id": "d6TFna9AKVgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "mahwpDsnKVdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "oHf4edXXKJGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Only pick Apple filings ending with _htm.xml\n",
        "files = sorted(glob.glob(\"aapl-*_htm.xml\"))\n",
        "\n",
        "print(f\"\u2705 Found {len(files)} files:\")\n",
        "for f in files:\n",
        "    print(\" -\", f)\n"
      ],
      "metadata": {
        "id": "0UsP6P1NKiII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import glob\n",
        "\n",
        "# Collect only aapl-*_htm.xml files in current directory\n",
        "xml_files = sorted(glob.glob(\"aapl-*_htm.xml\"))\n",
        "\n",
        "if not xml_files:\n",
        "    print(\"\u26a0\ufe0f No matching files found.\")\n",
        "else:\n",
        "    print(f\"\u2705 Found {len(xml_files)} files:\")\n",
        "    for f in xml_files:\n",
        "        print(f\" - {f}\")\n",
        "\n",
        "# Dictionary to hold all tags per file\n",
        "all_file_tags = {}\n",
        "\n",
        "# Process each file\n",
        "for file in xml_files:\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f, \"xml\")\n",
        "\n",
        "    # Get all unique tag names\n",
        "    tags = sorted({tag.name for tag in soup.find_all()})\n",
        "    all_file_tags[file] = tags\n",
        "\n",
        "# Display all tags found in each file\n",
        "for file, tags in all_file_tags.items():\n",
        "    print(f\"\\n\u2705 {file} has {len(tags)} unique tags:\")\n",
        "    print(tags)\n"
      ],
      "metadata": {
        "id": "gpd2cvAoKiEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import glob\n",
        "\n",
        "# Prepare list of tags we care about\n",
        "kpi_tags = {\n",
        "    \"Revenue\": \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
        "    \"NetIncome\": \"NetIncomeLoss\",\n",
        "    \"Assets\": \"Assets\",\n",
        "    \"Liabilities\": \"Liabilities\",\n",
        "    \"Equity\": \"StockholdersEquity\",\n",
        "    \"LongTermDebt\": \"LongTermDebtNoncurrent\"\n",
        "}\n",
        "\n",
        "# Collect only aapl-*_htm.xml files\n",
        "xml_files = sorted(glob.glob(\"aapl-*_htm.xml\"))\n",
        "\n",
        "if not xml_files:\n",
        "    print(\"\u26a0\ufe0f No matching files found.\")\n",
        "else:\n",
        "    print(f\"\u2705 Found {len(xml_files)} files to process:\")\n",
        "    for f in xml_files:\n",
        "        print(f\" - {f}\")\n",
        "\n",
        "# List to hold data rows\n",
        "data = []\n",
        "\n",
        "# Process each file\n",
        "for file in xml_files:\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f, \"xml\")\n",
        "\n",
        "    row = {\"File\": file}\n",
        "    for kpi, tag in kpi_tags.items():\n",
        "        element = soup.find(tag)\n",
        "        value = element.text.strip() if element and element.text else None\n",
        "        row[kpi] = value\n",
        "    data.append(row)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "haPjz39QKiCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import glob\n",
        "\n",
        "# Prepare list of tags we care about\n",
        "kpi_tags = {\n",
        "    \"Revenue\": \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
        "    \"NetIncome\": \"NetIncomeLoss\",\n",
        "    \"Assets\": \"Assets\",\n",
        "    \"Liabilities\": \"Liabilities\",\n",
        "    \"Equity\": \"StockholdersEquity\",\n",
        "    \"LongTermDebt\": \"LongTermDebtNoncurrent\",\n",
        "    \"FiscalYear\": \"DocumentFiscalYearFocus\"\n",
        "}\n",
        "\n",
        "\n",
        "# Collect only aapl-*_htm.xml files\n",
        "xml_files = sorted(glob.glob(\"aapl-*_htm.xml\"))\n",
        "data = []\n",
        "\n",
        "for file in xml_files:\n",
        "    with open(file, \"r\") as f:\n",
        "        soup = BeautifulSoup(f, \"xml\")\n",
        "\n",
        "    row = {\"File\": file}\n",
        "    for kpi, tag in kpi_tags.items():\n",
        "        element = soup.find(tag)\n",
        "        if element:\n",
        "            raw_value = element.text.strip()\n",
        "            clean_value = raw_value.replace(\",\", \"\").replace(\"$\", \"\")\n",
        "            try:\n",
        "                numeric_value = float(clean_value)\n",
        "            except ValueError:\n",
        "                numeric_value = None\n",
        "        else:\n",
        "            numeric_value = None\n",
        "        row[kpi] = numeric_value\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df\n"
      ],
      "metadata": {
        "id": "5RxbM2H5KiAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Convert KPI columns to numeric\n",
        "kpi_cols = [\"Revenue\", \"NetIncome\", \"Assets\", \"Liabilities\", \"Equity\"]\n",
        "for col in kpi_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Extract year\n",
        "df[\"Year\"] = df[\"File\"].str.extract(r\"(\\d{4})\").astype(int)\n",
        "df_sorted = df.sort_values(\"Year\")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Custom color palette\n",
        "colors = [\"#1f77b4\", \"#2ca02c\", \"#d62728\"]\n",
        "\n",
        "# Add one bar trace per year\n",
        "for i, (_, row) in enumerate(df_sorted.iterrows()):\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=kpi_cols,\n",
        "            y=[row[kpi] for kpi in kpi_cols],\n",
        "            name=str(row[\"Year\"]),\n",
        "            marker_color=colors[i]\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Update layout for clarity\n",
        "fig.update_layout(\n",
        "    title=\"Apple Financial KPIs by Year\",\n",
        "    xaxis_title=\"KPI\",\n",
        "    yaxis_title=\"Amount (USD Millions)\",\n",
        "    barmode=\"group\",\n",
        "    legend_title=\"Fiscal Year\",\n",
        "    template=\"plotly_white\",\n",
        "    font=dict(size=13)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "SLQAAM8RKh9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A7TH-brSKJD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yEjhb71Kyyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8n8SZcPKyvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oR7FLaPuKys1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJkk91vkKyqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwktE_QAKyn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hSzZnHeKylj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qLlcx59KyjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0W2s-FHyKygd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZouKUVWCKyeI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}